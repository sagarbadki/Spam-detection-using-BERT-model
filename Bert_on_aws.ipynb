{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (3.0.2)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.8.1rc1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2020.7.14)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (20.3)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (4.44.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from packaging->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling data from S3 Buckets.\n",
    "bucket='bert23'\n",
    "prefix='sagemaker/demos'\n",
    "\n",
    "sagemaker_session=sagemaker.Session()\n",
    "role=get_execution_role()\n",
    "\n",
    "s3_data_path='{}/{}/data'.format(bucket,prefix)\n",
    "se_output_path='{}/{}/output'.format(bucket,prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"s3://bert23/spamdata_v2.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.865937\n",
       "1    0.134063\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df['label'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train dataset into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text, temp_text, train_labels, temp_labels = train_test_split(df['text'], df['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=df['label'])\n",
    "\n",
    "# we will use temp_text and temp_labels to create validation and test set\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download BERT Model and BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fdc94941828>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get length of all the messages in the train set\n",
    "seq_len = [len(i.split()) for i in train_text]\n",
    "\n",
    "pd.Series(seq_len).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and encode sequences in the training set\n",
    "tokens_train = tokenizer.batch_encode_plus(\n",
    "    train_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the validation set\n",
    "tokens_val = tokenizer.batch_encode_plus(\n",
    "    val_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "# tokenize and encode sequences in the test set\n",
    "tokens_test = tokenizer.batch_encode_plus(\n",
    "    test_text.tolist(),\n",
    "    max_length = max_seq_len,\n",
    "    pad_to_max_length=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Integer Sequences to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train set\n",
    "train_seq = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask = torch.tensor(tokens_train['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "# for validation set\n",
    "val_seq = torch.tensor(tokens_val['input_ids'])\n",
    "val_mask = torch.tensor(tokens_val['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "# for test set\n",
    "test_seq = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask = torch.tensor(tokens_test['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "#define a batch size\n",
    "batch_size = 32\n",
    "\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "\n",
    "# dataLoader for train set\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# wrap tensors\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "\n",
    "# sampler for sampling the data during training\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "\n",
    "# dataLoader for validation set\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freeze BERT Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "      super(BERT_Arch, self).__init__()\n",
    "\n",
    "      self.bert = bert \n",
    "      \n",
    "      # dropout layer\n",
    "      self.dropout = nn.Dropout(0.1)\n",
    "      \n",
    "      # relu activation function\n",
    "      self.relu =  nn.ReLU()\n",
    "\n",
    "      # dense layer 1\n",
    "      self.fc1 = nn.Linear(768,512)\n",
    "      \n",
    "      # dense layer 2 (Output layer)\n",
    "      self.fc2 = nn.Linear(512,2)\n",
    "\n",
    "      #softmax activation function\n",
    "      self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "      #pass the inputs to the model  \n",
    "      _, cls_hs = self.bert(sent_id, attention_mask=mask)\n",
    "      \n",
    "      x = self.fc1(cls_hs)\n",
    "\n",
    "      x = self.relu(x)\n",
    "\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # output layer\n",
    "      x = self.fc2(x)\n",
    "      \n",
    "      # apply softmax activation\n",
    "      x = self.softmax(x)\n",
    "\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer from hugging face transformers\n",
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57743559 3.72848948]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/sklearn/utils/validation.py:71: FutureWarning: Pass classes=[0 1], y=1188    0\n",
      "5403    0\n",
      "2924    0\n",
      "880     1\n",
      "719     0\n",
      "       ..\n",
      "5121    0\n",
      "1006    1\n",
      "4374    1\n",
      "903     0\n",
      "5057    0\n",
      "Name: label, Length: 3900, dtype: int64 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "#compute the class weights\n",
    "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n",
    "\n",
    "print(class_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class weights to tensor\n",
    "weights= torch.tensor(class_wts,dtype=torch.float)\n",
    "\n",
    "\n",
    "# loss function\n",
    "cross_entropy  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# number of training epochs\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "  \n",
    "  model.train()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save model predictions\n",
    "  total_preds=[]\n",
    "  \n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "    # progress update after every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [r for r in batch]\n",
    " \n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # clear previously calculated gradients \n",
    "    model.zero_grad()        \n",
    "\n",
    "    # get model predictions for the current batch\n",
    "    preds = model(sent_id, mask)\n",
    "\n",
    "    # compute the loss between actual and predicted values\n",
    "    loss = cross_entropy(preds, labels)\n",
    "\n",
    "    # add on to the total loss\n",
    "    total_loss = total_loss + loss.item()\n",
    "\n",
    "    # backward pass to calculate the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # model predictions are stored on GPU. So, push it to CPU\n",
    "    preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "  # compute the training loss of the epoch\n",
    "  avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  #returns the loss and predictions\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for evaluating the model\n",
    "def evaluate():\n",
    "  \n",
    "  print(\"\\nEvaluating...\")\n",
    "  \n",
    "  # deactivate dropout layers\n",
    "  model.eval()\n",
    "\n",
    "  total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "  # empty list to save the model predictions\n",
    "  total_preds = []\n",
    "\n",
    "  # iterate over batches\n",
    "  for step,batch in enumerate(val_dataloader):\n",
    "    \n",
    "    # Progress update every 50 batches.\n",
    "    if step % 50 == 0 and not step == 0:\n",
    "      \n",
    "      # Calculate elapsed time in minutes.\n",
    "      elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "      # Report progress.\n",
    "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "    # push the batch to gpu\n",
    "    batch = [t for t in batch]\n",
    "\n",
    "    sent_id, mask, labels = batch\n",
    "\n",
    "    # deactivate autograd\n",
    "    with torch.no_grad():\n",
    "      \n",
    "      # model predictions\n",
    "      preds = model(sent_id, mask)\n",
    "\n",
    "      # compute the validation loss between actual and predicted values\n",
    "      loss = cross_entropy(preds,labels)\n",
    "\n",
    "      total_loss = total_loss + loss.item()\n",
    "\n",
    "      preds = preds.detach().cpu().numpy()\n",
    "\n",
    "      total_preds.append(preds)\n",
    "\n",
    "  # compute the validation loss of the epoch\n",
    "  avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "  # reshape the predictions in form of (number of samples, no. of classes)\n",
    "  total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "  return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.618\n",
      "Validation Loss: 0.510\n",
      "\n",
      " Epoch 2 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.339\n",
      "Validation Loss: 0.292\n",
      "\n",
      " Epoch 3 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.261\n",
      "Validation Loss: 0.214\n",
      "\n",
      " Epoch 4 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.248\n",
      "Validation Loss: 0.181\n",
      "\n",
      " Epoch 5 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.219\n",
      "Validation Loss: 0.234\n",
      "\n",
      " Epoch 6 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.207\n",
      "Validation Loss: 0.172\n",
      "\n",
      " Epoch 7 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.253\n",
      "Validation Loss: 0.163\n",
      "\n",
      " Epoch 8 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.185\n",
      "Validation Loss: 0.214\n",
      "\n",
      " Epoch 9 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.195\n",
      "Validation Loss: 0.137\n",
      "\n",
      " Epoch 10 / 10\n",
      "  Batch    50  of    122.\n",
      "  Batch   100  of    122.\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.211\n",
      "Validation Loss: 0.133\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, _ = evaluate()\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions for test data\n",
    "with torch.no_grad():\n",
    "  preds = model(test_seq, test_mask)\n",
    "  preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98       724\n",
      "           1       0.80      0.96      0.87       112\n",
      "\n",
      "    accuracy                           0.96       836\n",
      "   macro avg       0.90      0.96      0.93       836\n",
      "weighted avg       0.97      0.96      0.96       836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model's performance\n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>698</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0    0    1\n",
       "row_0          \n",
       "0      698   26\n",
       "1        5  107"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "pd.crosstab(test_y, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
